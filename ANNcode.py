# -*- coding: utf-8 -*-
"""ANN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P26Mzb-Mt8-pPRATUTDD8s4xcubBX8sq
"""

# 1. IMPORT LIBRARIES
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, roc_auc_score, roc_curve

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping

# 2. LOAD DATA
try:
    dataset = pd.read_csv('Churn_Modelling.csv')
    print("Dataset loaded successfully.")
except FileNotFoundError:
    print("Error: 'Churn_Modelling.csv' not found. Please upload the file.")

# 3. DATA PREPROCESSING

# A. Feature Selection
X = dataset.iloc[:, 3:-1].values
y = dataset.iloc[:, -1].values

# B. Encoding Categorical Data
le = LabelEncoder()
X[:, 2] = le.fit_transform(X[:, 2]) # Gender

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')
X = np.array(ct.fit_transform(X)) # Geography

# 4. DATA SPLITTING (80/20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# 5. FEATURE SCALING
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# 6. MODEL ARCHITECTURE (EXPANSION STRATEGY)
model = Sequential()

# Hidden Layer 1:
# We use 18 neurons to project data into higher dimensions (Cover's Theorem)
model.add(Dense(units=6, activation='relu', kernel_regularizer=l2(0.0), input_dim=X_train.shape[1]))
model.add(Dropout(0)) # 10% Dropout

# Hidden Layer 2:
# We compress the features to summarize the most important patterns
model.add(Dense(units=6, activation='relu', kernel_regularizer=l2(0.0)))
model.add(Dropout(0)) # 10% Dropout

# Output Layer
model.add(Dense(units=1, activation='sigmoid'))

# Compile
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 7. MODEL TRAINING
print("\nTraining model...")
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

history = model.fit(
    X_train, y_train,
    batch_size=32,
    epochs=100,
    validation_data=(X_test, y_test),
    callbacks=[early_stop],
    verbose=1
)

# 8. EVALUATION & METRICS
print("\nCalculating Metrics...")

# Get Predictions
y_pred_probs = model.predict(X_test)
y_pred = (y_pred_probs > 0.5)

# Calculate 3 Key Metrics
acc = accuracy_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
roc = roc_auc_score(y_test, y_pred_probs)

print(f"--------------------------------------")
print(f"Final Test Accuracy: {acc*100:.2f}%")
print(f"Recall (Sensitivity): {rec*100:.2f}%")
print(f"ROC-AUC Score:       {roc:.4f}")
print(f"--------------------------------------")

# 9. VISUALIZATIONS
plt.figure(figsize=(18, 5))

# Plot A: Loss Curve
plt.subplot(1, 3, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Learning Curve (Loss)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# Plot B: Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.subplot(1, 3, 2)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')

# Plot C: ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)
plt.subplot(1, 3, 3)
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc:.2f}')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.grid(True)

plt.tight_layout()
plt.show()